{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.1 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "0d008810b9c8467bcb3ca39aa2180e5b81b3a9acb136aab30d47954377cc5120"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 309/309 [00:29<00:00, 10.42it/s]\n",
      "100%|██████████| 373/373 [00:33<00:00, 11.09it/s]\n",
      "100%|██████████| 245/245 [00:24<00:00,  9.81it/s]\n",
      "100%|██████████| 413/413 [00:37<00:00, 10.96it/s]\n",
      "100%|██████████| 274/274 [00:25<00:00, 10.83it/s]\n",
      "100%|██████████| 518/518 [00:47<00:00, 10.93it/s]\n",
      "100%|██████████| 390/390 [00:35<00:00, 11.02it/s]\n",
      "100%|██████████| 764/764 [01:10<00:00, 10.89it/s]\n",
      "100%|██████████| 90/90 [00:08<00:00, 11.05it/s]\n",
      "100%|██████████| 326/326 [00:32<00:00,  9.97it/s]\n",
      "time cost: 360.7176661491394 s\n",
      "total article num: 3702\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import re\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def read_chr_link(mw_category_group,href,title):\n",
    "    chr_links=mw_category_group.findChildren(name='a')\n",
    "    for i in range(len(chr_links)): \n",
    "        href.append('https://en.wikinews.org'+chr_links[i]['href'])\n",
    "        title.append(chr_links[i].text)\n",
    "\n",
    "def pagesearch(page_url,href,title):\n",
    "    chr_contents=requests.get(page_url).text\n",
    "    chr_soup= BeautifulSoup(chr_contents, 'html.parser')\n",
    "    mw_pages=chr_soup.find(id=\"mw-pages\")\n",
    "    mw_category=mw_pages.findAll(name='div',attrs={\"class\":re.compile(r'mw-category')})[0]\n",
    "    category_num=len(mw_category.findChildren(name='div',attrs={\"class\":re.compile(r'mw-category-group')}))\n",
    "    mw_category_group=mw_pages.findAll(name='div',attrs={\"class\":re.compile(r'mw-category-group')})[0]\n",
    "    if(category_num<=1):\n",
    "        read_chr_link(mw_category_group,href,title)\n",
    "        hreflinks=mw_pages.findAll(name='a',attrs={\"title\":re.compile(r'Category:Politics and conflicts')})\n",
    "        if hreflinks[1]: next_page_url=re.sub(r'amp;','','https://en.wikinews.org'+hreflinks[1]['href'])\n",
    "        elif hreflinks[3]: next_page_url=re.sub(r'amp;','','https://en.wikinews.org'+hreflinks[3]['href'])\n",
    "        #print(\"category num:\",category_num,\",move to next page\")\n",
    "        pagesearch(next_page_url,href,title)\n",
    "    else:\n",
    "        read_chr_link(mw_category_group,href,title)\n",
    "        #print(\"category num:\",category_num,\",not move to next page\")\n",
    "    return href\n",
    "\n",
    "time_start=time.time()\n",
    "r = requests.get('https://en.wikinews.org/wiki/Category:Politics_and_conflicts')\n",
    "contents = r.text\n",
    "news_row = []\n",
    "for i in contents.split('<'):\n",
    "    chrlink=re.match(r'a class=\\\"external text\\\" href=\\\"(.*)\\\">(.+)',i)\n",
    "    if chrlink:\n",
    "        if chrlink.group(2) in ['M','N','O','P','R','S','T','U','V','W']:\n",
    "            href_list=[]; title_list=[]\n",
    "            href_list=pagesearch(re.sub(r'amp;','',chrlink.group(1)),href_list,title_list)\n",
    "            #print(\"char:\",chrlink.group(2),\"\\tnum:\",len(href_list))\n",
    "            #print(\"first article:\",href_list[0])\n",
    "            #print(\"last article:\",href_list[len(href_list)-1])\n",
    "            for j in tqdm(range(len(href_list))):\n",
    "                news_page = BeautifulSoup(requests.get(href_list[j]).text, 'html.parser').find(class_=\"mw-parser-output\")\n",
    "                date = [idate.get_text() for idate in news_page.findAll(name='strong',attrs={\"class\":re.compile(r'published')})]\n",
    "                date = ''.join(date)\n",
    "                #print(j,href_list[j])\n",
    "                content = [icontent.get_text() for icontent in news_page.findAll(name='p')[1:]]\n",
    "                content = ' '.join(content)\n",
    "                #content = ''.join(content.split('\\n'))\n",
    "                content = content.replace('\\n', ' ').replace('\\r', ' ')\n",
    "                news_row.append([chrlink.group(2),j+1,href_list[j],title_list[j],date,content])\n",
    "\n",
    "time_end=time.time()\n",
    "print('\\ntime cost:',time_end-time_start,'s')\n",
    "print(\"total article num:\",len(news_row))\n",
    "\n",
    "output = open('news1.xls','w',encoding='utf-8')\n",
    "output.write('char\\tnum\\thref\\ttitle\\tdate\\tcontent\\n')\n",
    "for i in range(len(news_row)):\n",
    "\tfor j in range(len(news_row[i])):\n",
    "\t\toutput.write(str(news_row[i][j]))\n",
    "\t\toutput.write('\\t')\n",
    "\toutput.write('\\n')\n",
    "output.close()\n",
    "\n",
    "#print(news_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}